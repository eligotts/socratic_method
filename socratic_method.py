import asyncio
import json
import math
import os
import re
from textwrap import dedent
from typing import Any, Dict

from datasets import Dataset, load_dataset
from openai import AsyncOpenAI

import verifiers as vf


DEFAULT_SYSTEM_PROMPT = dedent(
    """
    You are an expert at the Socratic method. Use the provided context to reason about and then generate the next line of dialogue. 
    Follow the requested <think>/<answer> format exactly.
    You thinking section MUST be wrapped in <think></think>, and your answer section MUST be wrapped in <answer></answer>.
    Example response:
    <think>
    Your thinking here
    </think>
    <answer>
    Your answer here
    </answer>
    """
).strip()

PROMPT_TEMPLATE = dedent(
    """
    Here is the overall argumentation strategy: {global_sketch}
    Here are the conceded premises: {conceded_premises}
    Here is interlocutor profile: {interlocutor_profile}
    Here is the dialogue summary: {argument_history_summary}
    Here are the last lines of dialogue:
    {dialogue_last_turns}

    Wrap in <think></think> as you consider what to say next. Think about which
    premises to target, how this next line advances the argument, and be explicit
    about the direction you intend to take the argument given the line you come up with.
    Wrap in <answer></answer> with exactly the next line of dialogue you come up with.
    """
).strip()

def _build_shared_judge_prompt(
    info: Dict[str, Any],
    think_block: str,
    predicted_answer: str,
    answer: str,
) -> str:
    """Build the shared prompt with all context information."""
    metadata = info or {}
    return dedent(
        f"""
        You are evaluating a line of dialogue in a Socratic dialogue. Compare with the ground truth dialogue line and judge how well it matches the ground truth line's argumentation move.

        Context
        -------
        Overall argumentation strategy: {metadata.get("global_sketch", "")}
        Conceded premises: {metadata.get("conceded_premises", "")}
        Interlocutor profile: {metadata.get("interlocutor_profile", "")}
        Dialogue summary: {metadata.get("argument_history_summary", "")}
        Last dialogue turns:
        {metadata.get("dialogue_last_turns", "")}

        Ground truth Socrates line: {answer}

        Information on the ground truth Socrates line:
        Abstract objective (how the line advances the argument): {metadata.get("abstract_objective", "")}
        Key premises targeted: {metadata.get("key_premises_targeted", "")}
        Tactic employed in the line of dialogue: {metadata.get("socratic_tactic_employed", "")}
        Rationale (larger scope understanding of how this line of dialogue fits into the argumentation strategy): {metadata.get("rationale", "")}

        Content you are evaluating:
        Thought process behind the predicted line of dialogue:
        ---
        {think_block}
        ---

        Predicted line of dialogue:
        ---
        {predicted_answer}
        ---
        """
    ).strip()


async def _make_judge_call(
    prompt: str,
    score_request: str,
    judge_client: AsyncOpenAI,
    judge_model: str,
    judge_sampling_args: Dict[str, Any] | None = None,
) -> float:
    """Make a single judge model call and parse the float score."""
    full_prompt = prompt + "\n\n" + score_request + "\n\nDO NOT return anything else but the single float score. /no_think"
    
    try:
        response = await judge_client.chat.completions.create(
            model=judge_model,
            messages=[{"role": "user", "content": full_prompt}],
            **_normalize_sampling_args(judge_sampling_args),
        )
    except Exception:
        return 0.0
    
    message = response.choices[0].message.content if response.choices else None

    if not message:
        return 0.0
    
    # Parse the float score
    try:
        # Strip whitespace and any surrounding text
        score_text = message.strip()
        # Try to extract just the number if it's in a sentence
        match = re.search(r'\b([0-9]*\.?[0-9]+)\b', score_text)
        if match:
            score = float(match.group(1))
            return _clamp_score(score)
    except (ValueError, AttributeError):
        pass
    
    return 0.0


def _format_iterable(value: Any) -> str:
    if isinstance(value, list):
        items = []
        for item in value:
            if isinstance(item, dict):
                speaker = str(item.get("speaker") or item.get("role") or "").strip()
                text = str(item.get("text") or item.get("content") or "").strip()
                if speaker and text:
                    items.append(f"{speaker}: {text}")
                elif text:
                    items.append(text)
                elif speaker:
                    items.append(speaker)
            else:
                text = str(item).strip()
                if text:
                    items.append(text)
        return "\n".join(items)
    return str(value).strip()


def _stringify(value: Any) -> str:
    if value is None:
        return ""
    text = _format_iterable(value)
    return text if text else ""

def _normalize_sampling_args(sampling_args: Dict[str, Any] | None) -> Dict[str, Any]:
    if sampling_args is None:
        return {}
    normalized = dict(sampling_args)
    if "max_tokens" in normalized:
        normalized["max_completion_tokens"] = normalized.pop("max_tokens")
    if normalized.get("max_completion_tokens") is None:
        normalized.pop("max_completion_tokens", None)
    return {k: v for k, v in normalized.items() if v is not None}




def _clamp_score(value: float) -> float:
    return max(0.0, min(1.0, value))



# Calculate the embedding similarity between the predicted and target
async def answer_embedding_similarity_reward(
    completion: list[Dict[str, Any]],
    answer: str,
    parser: vf.Parser,
    embed_client: AsyncOpenAI,
    embed_model: str,
    **kwargs: Any,
) -> float:
    predicted = parser.parse_answer(completion) or ""
    target = answer or ""
    if not predicted.strip() or not target.strip():
        return 0.0

    try:
        predicted_task = embed_client.embeddings.create(
            model=embed_model,
            input=[predicted],
        )
        target_task = embed_client.embeddings.create(
            model=embed_model,
            input=[target],
        )
        predicted_response, target_response = await asyncio.gather(
            predicted_task, target_task
        )
    except Exception:
        return 0.0

    try:
        predicted_vector = predicted_response.data[0].embedding  # type: ignore
        target_vector = target_response.data[0].embedding  # type: ignore
    except (AttributeError, IndexError, KeyError):
        return 0.0

    if not predicted_vector or not target_vector:
        return 0.0

    dot = sum(p * t for p, t in zip(predicted_vector, target_vector))
    pred_norm = math.sqrt(sum(p * p for p in predicted_vector))
    targ_norm = math.sqrt(sum(t * t for t in target_vector))
    if pred_norm == 0.0 or targ_norm == 0.0:
        return 0.0
    cosine = dot / (pred_norm * targ_norm)
    # Map cosine similarity [-1, 1] to [0, 1]
    return max(0.0, min(1.0, 0.5 * (cosine + 1.0)))




async def think_premise_alignment_reward(
    completion: list[Dict[str, Any]],
    answer: str,
    info: Dict[str, Any],
    parser: vf.Parser,
    judge_client: AsyncOpenAI,
    judge_model: str,
    judge_sampling_args: Dict[str, Any] | None = None,
    **kwargs: Any,
) -> float:
    assistant_messages = parser.get_assistant_messages(completion)
    if not assistant_messages:
        return 0.0
    parsed_message = parser.parse(assistant_messages[-1]["content"])
    think_text = getattr(parsed_message, "think", None)
    predicted_answer = getattr(parsed_message, "answer", None)
    
    if not think_text or not predicted_answer:
        return 0.0
    
    prompt = _build_shared_judge_prompt(info, think_text, predicted_answer, answer)
    score_request = (
        "Score the premise_alignment (0.0 to 1.0): does the thought recall and leverage "
        "the same conceded premises and targeted premises as the ground truth 'key premises targeted'? "
        "Be very discerning. High scores (0.75+) should only come from PERFECT ALIGNMENT with the ground truth.\n\n"
        "Return only the float score:"
    )
    
    return await _make_judge_call(prompt, score_request, judge_client, judge_model, judge_sampling_args)


async def think_objective_alignment_reward(
    completion: list[Dict[str, Any]],
    answer: str,
    info: Dict[str, Any],
    parser: vf.Parser,
    judge_client: AsyncOpenAI,
    judge_model: str,
    judge_sampling_args: Dict[str, Any] | None = None,
    **kwargs: Any,
) -> float:
    assistant_messages = parser.get_assistant_messages(completion)
    if not assistant_messages:
        return 0.0
    parsed_message = parser.parse(assistant_messages[-1]["content"])
    think_text = getattr(parsed_message, "think", None)
    predicted_answer = getattr(parsed_message, "answer", None)
    
    if not think_text or not predicted_answer:
        return 0.0
    
    prompt = _build_shared_judge_prompt(info, think_text, predicted_answer, answer)
    score_request = (
        "Score the objective_alignment (0.0 to 1.0): is the plan oriented toward the abstract objective "
        "and rationale for this move? Compare the ground truth 'abstract objective' and 'rationale' to the thought. "
        "Be very discerning. High scores (0.75+) should only come from PERFECT ALIGNMENT with the ground truth.\n\n"
        "Return only the float score:"
    )
    
    return await _make_judge_call(prompt, score_request, judge_client, judge_model, judge_sampling_args)


async def think_tactic_consistency_reward(
    completion: list[Dict[str, Any]],
    answer: str,
    info: Dict[str, Any],
    parser: vf.Parser,
    judge_client: AsyncOpenAI,
    judge_model: str,
    judge_sampling_args: Dict[str, Any] | None = None,
    **kwargs: Any,
) -> float:
    assistant_messages = parser.get_assistant_messages(completion)
    if not assistant_messages:
        return 0.0
    parsed_message = parser.parse(assistant_messages[-1]["content"])
    think_text = getattr(parsed_message, "think", None)
    predicted_answer = getattr(parsed_message, "answer", None)
    
    if not think_text or not predicted_answer:
        return 0.0
    
    prompt = _build_shared_judge_prompt(info, think_text, predicted_answer, answer)
    score_request = (
        "Score the tactic_consistency (0.0 to 1.0): is the proposed approach consistent with the ground truth "
        "Socratic tactic? "
        "Be very discerning. High scores (0.75+) should only come from PERFECT ALIGNMENT with the ground truth.\n\n"
        "Return only the float score:"
    )
    
    return await _make_judge_call(prompt, score_request, judge_client, judge_model, judge_sampling_args)



async def answer_semantic_fidelity_reward(
    completion: list[Dict[str, Any]],
    answer: str,
    info: Dict[str, Any],
    parser: vf.Parser,
    judge_client: AsyncOpenAI,
    judge_model: str,
    judge_sampling_args: Dict[str, Any] | None = None,
    **kwargs: Any,
) -> float:
    assistant_messages = parser.get_assistant_messages(completion)
    if not assistant_messages:
        return 0.0
    parsed_message = parser.parse(assistant_messages[-1]["content"])
    think_text = getattr(parsed_message, "think", None)
    predicted_answer = getattr(parsed_message, "answer", None)
    
    if not think_text or not predicted_answer:
        return 0.0
    
    prompt = _build_shared_judge_prompt(info, think_text, predicted_answer, answer)
    score_request = (
        "Score the semantic_fidelity (0.0 to 1.0): how well does the predicted line of dialogue match the meaning and argumentative force "
        "of the ground truth? "
        "Be very discerning. High scores (0.75+) should only come from perfect alignment with the ground truth.\n\n"
        "Return only the float score:"
    )
    
    return await _make_judge_call(prompt, score_request, judge_client, judge_model, judge_sampling_args)


async def answer_tactic_alignment_reward(
    completion: list[Dict[str, Any]],
    answer: str,
    info: Dict[str, Any],
    parser: vf.Parser,
    judge_client: AsyncOpenAI,
    judge_model: str,
    judge_sampling_args: Dict[str, Any] | None = None,
    **kwargs: Any,
) -> float:
    assistant_messages = parser.get_assistant_messages(completion)
    if not assistant_messages:
        return 0.0
    parsed_message = parser.parse(assistant_messages[-1]["content"])
    think_text = getattr(parsed_message, "think", None)
    predicted_answer = getattr(parsed_message, "answer", None)
    
    if not think_text or not predicted_answer:
        return 0.0
    
    prompt = _build_shared_judge_prompt(info, think_text, predicted_answer, answer)
    score_request = (
        "Score the answer_tactic_alignment (0.0 to 1.0): does the predicted line of dialogue adhere to the Socratic tactic?"
        "Be very discerning. High scores (0.75+) should only come from perfect alignment with the ground truth.\n\n"
        "Return only the float score:"
    )
    
    return await _make_judge_call(prompt, score_request, judge_client, judge_model, judge_sampling_args)



def _prepare_dataset(  # type: ignore[override]
    dataset: Dataset,
    system_prompt: str,
) -> Dataset:
    def _prepare(example: Dict[str, Any]) -> Dict[str, Any]:
        global_sketch = _stringify(example.get("global_sketch"))
        conceded_premises = _stringify(example.get("conceded_premises"))
        interlocutor_profile = _stringify(example.get("interlocutor_profile"))
        argument_history_summary = _stringify(example.get("argument_history_summary"))
        dialogue_last_turns = _stringify(example.get("dialogue_last_turns_window"))
        abstract_objective = _stringify(example.get("abstract_objective"))
        key_premises_targeted = _stringify(example.get("key_premises_targeted"))
        socratic_tactic_employed = _stringify(example.get("socratic_tactic_employed"))
        rationale = _stringify(example.get("rationale"))
        ground_truth = _stringify(example.get("socrates_completion"))

        user_prompt = PROMPT_TEMPLATE.format(
            global_sketch=global_sketch,
            conceded_premises=conceded_premises,
            interlocutor_profile=interlocutor_profile,
            argument_history_summary=argument_history_summary,
            dialogue_last_turns=dialogue_last_turns,
            socratic_tactic_employed=socratic_tactic_employed,
            abstract_objective=abstract_objective,
            key_premises_targeted=key_premises_targeted,
            rationale=rationale,
        )

        example["prompt"] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
        example["answer"] = ground_truth
        example["info"] = {
            "global_sketch": global_sketch,
            "conceded_premises": conceded_premises,
            "interlocutor_profile": interlocutor_profile,
            "argument_history_summary": argument_history_summary,
            "dialogue_last_turns": dialogue_last_turns,
            "abstract_objective": abstract_objective,
            "key_premises_targeted": key_premises_targeted,
            "socratic_tactic_employed": socratic_tactic_employed,
            "rationale": rationale,
            "ground_truth": ground_truth,
        }
        return example

    return dataset.map(_prepare)


def load_environment(
    dataset_path: str = "ergotts/socratic-method",
    train_split: str | None = "train",
    eval_split: str | None = None,
    num_train_examples: int = -1,
    num_eval_examples: int = -1,
    holdout_seed: int = 42,
    system_prompt: str | None = None,
    judge_model: str = "Qwen/Qwen3-8B",
    judge_base_url: str = "http://0.0.0.0:8002/v1",
    judge_api_key_var: str = "OPENROUTER_API_KEY",
    judge_sampling_args: Dict[str, Any] | None = None,
    embedding_model: str = "text-embedding-3-small",
    embedding_base_url: str = "https://api.openai.com/v1",
    embedding_api_key_var: str = "OPENAI_API_KEY",
    **env_kwargs: Any,
) -> vf.Environment:
    system_prompt = (system_prompt or DEFAULT_SYSTEM_PROMPT).strip()

    train_dataset: Dataset
    if train_split is None:
        dataset_dict = load_dataset(dataset_path)
        if "train" in dataset_dict:
            train_dataset = dataset_dict["train"]  # type: ignore[index]
        else:
            first_key = next(iter(dataset_dict.keys()))
            train_dataset = dataset_dict[first_key]  # type: ignore[index]
    else:
        train_dataset = load_dataset(dataset_path, split=train_split)  # type: ignore[assignment]

    if eval_split is not None:
        eval_dataset = load_dataset(dataset_path, split=eval_split)  # type: ignore[assignment]
    else:
        dataset_dict = load_dataset(dataset_path)
        if "validation" in dataset_dict:
            eval_dataset = dataset_dict["validation"]  # type: ignore[index]
        elif "test" in dataset_dict:
            eval_dataset = dataset_dict["test"]  # type: ignore[index]
        else:
            split = train_dataset.train_test_split(
                test_size=min(200, max(1, len(train_dataset) // 10)),
                seed=holdout_seed,
                shuffle=True,
            )
            train_dataset = split["train"]
            eval_dataset = split["test"]

    train_dataset = _prepare_dataset(train_dataset, system_prompt)
    eval_dataset = _prepare_dataset(eval_dataset, system_prompt)

    if num_train_examples != -1:
        train_dataset = train_dataset.select(
            range(min(num_train_examples, len(train_dataset)))
        )
    if num_eval_examples != -1:
        eval_dataset = eval_dataset.select(
            range(min(num_eval_examples, len(eval_dataset)))
        )

    parser = vf.XMLParser(["think", "answer"], answer_field="answer")

    judge_client = AsyncOpenAI(
        base_url=judge_base_url,
        api_key=os.getenv(judge_api_key_var, "EMPTY"),
    )
    embed_client = AsyncOpenAI(
        base_url=embedding_base_url,
        api_key=os.getenv(embedding_api_key_var, "EMPTY"),
    )

    embedding_rubric = vf.Rubric(
        funcs=[answer_embedding_similarity_reward, parser.get_format_reward_func()],
        weights=[0.8, 0.2],
        parser=parser,
        parallelize_scoring=False,
    )
    embedding_rubric.class_objects.update(
        {
            "embed_client": embed_client,
            "embed_model": embedding_model,
        }
    )

    combined_rubric = vf.Rubric(
        funcs=[
            think_premise_alignment_reward,
            think_objective_alignment_reward,
            think_tactic_consistency_reward,
            answer_semantic_fidelity_reward,
            answer_tactic_alignment_reward,
        ],
        weights=[1.0/5.0, 1.0/5.0, 1.0/5.0, 1.0/5.0, 1.0/5.0, 1.0/5.0],
        parser=parser,
        parallelize_scoring=False,
    )
    combined_rubric.class_objects.update(
        {
            "judge_client": judge_client,
            "judge_model": judge_model,
            "judge_sampling_args": judge_sampling_args or {"temperature": 0},
        }
    )

    rubric = vf.RubricGroup(
        [embedding_rubric, combined_rubric]
    )

    vf_env = vf.SingleTurnEnv(
        dataset=train_dataset,
        eval_dataset=eval_dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
        **env_kwargs,
    )

    return vf_env
